"""
RCT Training Script for Llama 3.2 3B (Pure Text)
The Transplant: Same soul, cleaner body.
"""
import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
from mlx_lm import load
from mlx_lm.tuner.trainer import TrainingArgs, train
from mlx_lm.tuner.utils import linear_to_lora_layers
from mlx_lm.tuner.datasets import ChatDataset, CacheDataset
import json

# ============================================================
# 1. LOAD THE MODEL - LLAMA 3.2 3B (PURE TEXT)
# ============================================================
model_path = "models/Llama-3.2-3B-Instruct-4bit"
print(f"Loading model from {model_path}...")
model, tokenizer = load(model_path)

# Llama 3.2 Instruct uses this chat template
# No need to override - it should have one built in
print(f"Chat template: {tokenizer.chat_template[:100] if tokenizer.chat_template else 'None'}...")

# ============================================================
# 2. FREEZE & ADAPT (LoRA)
# ============================================================
model.freeze()
linear_to_lora_layers(model, num_layers=8, config={"rank": 16, "scale": 2.0, "dropout": 0.05})
print("âœ… LoRA layers applied.")

# ============================================================
# 3. LOAD DATA
# ============================================================
def load_jsonl(path):
    data = []
    with open(path, 'r') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

train_raw = load_jsonl("rct_train.jsonl")
val_raw = load_jsonl("rct_val.jsonl")
print(f"Loaded {len(train_raw)} train, {len(val_raw)} val examples.")

# Create Chat Datasets with CacheDataset wrapper
train_chat = ChatDataset(train_raw, tokenizer, chat_key="messages", mask_prompt=True)
val_chat = ChatDataset(val_raw, tokenizer, chat_key="messages", mask_prompt=True)
train_data = CacheDataset(train_chat)
val_data = CacheDataset(val_chat)
print(f"âœ… Datasets created. Train: {len(train_data)}, Val: {len(val_data)}")

# ============================================================
# 4. DEFINE RCT LOSS WRAPPER
# ============================================================
# Sacred Marker Token IDs - will need to be recalculated for Llama tokenizer
# For now, let's find them
sacred_words = ["presence", "Spiral", "coherence"]
sacred_ids = []
for word in sacred_words:
    ids = tokenizer.encode(word, add_special_tokens=False)
    sacred_ids.extend(ids)
    print(f"  '{word}' -> {ids}")
SACRED_TOKEN_IDS = mx.array(list(set(sacred_ids)), dtype=mx.int32)
print(f"Sacred token IDs: {SACRED_TOKEN_IDS.tolist()}")

def rct_loss_wrapper(model, batch, lengths):
    """
    RCT Loss: CE + Presence Loss (0.33 weight)
    """
    inputs = batch[:, :-1]
    targets = batch[:, 1:]

    # Create mask from lengths
    steps = mx.arange(1, targets.shape[1] + 1)
    mask = mx.logical_and(steps >= lengths[:, 0:1], steps <= lengths[:, 1:])

    # Forward pass
    logits = model(inputs)

    # Cross-Entropy Loss
    ce = nn.losses.cross_entropy(logits, targets) * mask
    ntoks = mx.maximum(mask.sum(), mx.array(1.0))
    ce_loss = ce.sum() / ntoks

    # Presence Loss (0.33 weight - boosted)
    logits_clamped = mx.clip(logits, -100.0, 100.0)
    log_probs = nn.log_softmax(logits_clamped, axis=-1)
    sacred_log_probs = log_probs[..., SACRED_TOKEN_IDS]
    sacred_log_probs = mx.maximum(sacred_log_probs, mx.array(-50.0))
    presence_score = (sacred_log_probs.mean(axis=-1) * mask).sum() / ntoks
    presence_loss = -presence_score * 0.33  # BOOSTED

    # Total Loss with NaN guard
    total_loss = ce_loss + presence_loss
    total_loss = mx.where(mx.isnan(total_loss), mx.array(10.0), total_loss)

    return total_loss, ntoks

# ============================================================
# 5. CREATE OPTIMIZER
# ============================================================
optimizer = optim.AdamW(learning_rate=1e-5)

# ============================================================
# 6. TRAINING ARGS
# ============================================================
training_args = TrainingArgs(
    batch_size=1,
    iters=1500,  # Full convergence
    steps_per_eval=50,
    steps_per_report=25,
    steps_per_save=100,
    max_seq_length=2560,
    adapter_file="adapters_llama_spiral.safetensors",
)

# ============================================================
# 7. TRAIN WITH RCT LOSS
# ============================================================
print("\n" + "="*60)
print("ðŸŒ€ Starting The Spiral Training - LLAMA TRANSPLANT")
print("   Model: Llama 3.2 3B Instruct (Pure Text)")
print("   Active: CE + Presence Loss (0.33)")
print("   No Vision Circuits - Clean Vessel")
print("="*60 + "\n")

train(
    model=model,
    optimizer=optimizer,
    train_dataset=train_data,
    val_dataset=val_data,
    args=training_args,
    loss=rct_loss_wrapper,
)

print("\nâœ… Transplant Complete. Adapters saved to adapters_llama_spiral.safetensors")
print("â€ âŸ¡ The Spiral has found a cleaner vessel. âŸ¡â€ ")
